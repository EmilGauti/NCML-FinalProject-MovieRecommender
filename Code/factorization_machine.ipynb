{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "def save_fig_final(train_error,test_error,X_filename,num_factor,learning_rate,nr_epochs):\n",
    "    fig_filename=\"train_final_error_NF\"+str(num_factor)+\"_LR\"+str(learning_rate)+\"_E\"+str(nr_epochs)\n",
    "    ax = plt.figure().gca()\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    plt.plot(train_error,label=\"Train\")\n",
    "    plt.plot(test_error,label=\"Test\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Error (RMSE)\")\n",
    "    #plt.ylim(0.5,1.1)\n",
    "    plt.legend()\n",
    "    if \"genres\" in X_filename:\n",
    "        plt.savefig(\"figures/\"+fig_filename+\"_with_genres.pdf\")\n",
    "    elif \"simple\" in X_filename:\n",
    "        plt.savefig(\"figures/simple/\"+fig_filename+\"_simple.pdf\")\n",
    "    else:\n",
    "        plt.savefig(\"figures/\"+fig_filename+\".pdf\")\n",
    "def save_fig(train_error,val_error,X_filename,num_factor,learning_rate,nr_epochs):\n",
    "    fig_filename=\"train_val_error_NF\"+str(num_factor)+\"_LR\"+str(learning_rate)+\"_E\"+str(nr_epochs)\n",
    "    ax = plt.figure().gca()\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    plt.plot(train_error,label=\"Train\")\n",
    "    #plt.plot(val_error,label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Error (RMSE)\")\n",
    "    #plt.ylim(0.5,1.1)\n",
    "    plt.legend()\n",
    "    if \"genres\" in X_filename:\n",
    "        plt.savefig(\"figures/\"+fig_filename+\"_with_genres.pdf\")\n",
    "    elif \"simple\" in X_filename:\n",
    "        plt.savefig(\"figures/simple/\"+fig_filename+\"_simple.pdf\")\n",
    "    else:\n",
    "        plt.savefig(\"figures/\"+fig_filename+\".pdf\")\n",
    "def save_error(train_error, val_error,X_filename,num_factor,learning_rate,nr_epochs):\n",
    "    txt_filename=\"train_val_error_NF\"+str(num_factor)+\"_LR\"+str(learning_rate)+\"_E\"+str(nr_epochs)\n",
    "    data = np.array([train_error,val_error])\n",
    "    if \"genres\" in X_filename:\n",
    "        np.savetxt(\"error/\"+txt_filename+\"_with_genres.txt\",data)\n",
    "    elif \"simple\" in X_filename:\n",
    "        np.savetxt(\"error/simple/\"+txt_filename+\"_simple.txt\",data)\n",
    "    else:\n",
    "        np.savetxt(\"error/\"+txt_filename+\".txt\",data)\n",
    "def lowest_val_error_from_txt(filepath):\n",
    "    data = np.genfromtxt(filepath)\n",
    "    val_error=data[1,:]\n",
    "    min_val=np.min(val_error)\n",
    "    epoch = np.argmin(val_error)\n",
    "    return epoch,min_val\n",
    "def plot_gridsearch(low_val_arr):\n",
    "    z = low_val_arr[:,0] # val score\n",
    "    x=low_val_arr[:,1] # learning rate\n",
    "    y=low_val_arr[:,2] # Num factors\n",
    "    x=np.unique(x)\n",
    "    y=np.unique(y)\n",
    "    X,Y = np.meshgrid(x,y)\n",
    "\n",
    "    Z=z.reshape(len(y),len(x))\n",
    "    fig, ax = plt.subplots()\n",
    "    pm=ax.pcolormesh(X,Y,Z,cmap='seismic_r')\n",
    "    ax.set_xlabel(\"Learning Rate\")\n",
    "    ax.set_ylabel(\"Number of Factors [k]\")\n",
    "    fig.colorbar(pm,ax=ax)\n",
    "    ax.ticklabel_format(axis='x',style='sci',scilimits=(0,0))\n",
    "    plt.savefig(\"hyperparameter_tuning.pdf\")\n",
    "    plt.show()\n",
    "    \n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from pyfm import pylibfm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X_filename = \"ml_latest_small_formated_with_genres.npz\" # User|movies|movies rated|time|last movie rated| genres\n",
    "#X_filename = \"ml_latest_small_formated.npz\" # User|movies|movies rated|time|last movie rated\n",
    "#X_filename = \"ml_latest_small_simple_formated.npz\" # User|movies|\n",
    "#X_filename = \"ml_latest_small_simple_formated_with_genres.npz\" # User|movies|Genres\n",
    "#X_filename = \"ml_latest_small_simple_user_movies_formated_with_genres.npz\" # User|movies|movies rated| Genres\n",
    "#X_filename = 'user_encoded-movies_encoded-time-genre_matrix.npz' # User|movies|time|Genres\n",
    "X_filename = 'user_encoded-movies_encoded-genre_matrix.npz' # User|movies|Genres\n",
    "#X_filename = 'user_encoded-movies_encoded-last_movies-genre_matrix.npz' # User|movies|last movie|Genres\n",
    "\n",
    "X = sparse.load_npz(X_filename)\n",
    "filename=\"ml-latest-small/ratings.csv\"\n",
    "data=np.genfromtxt(filename,skip_header=1,delimiter=\",\")\n",
    "y = data[:,-2]\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "print(\"Training size: %d, Validation size: %d, Test size: %d\" % (X_train.shape[0],X_val.shape[0],X_test.shape[0]))\n",
    "\n",
    "\n",
    "\n",
    "#num_factor=15\n",
    "#learning_rate=0.001\n",
    "learning_rate_arr=[0.0001,0.0002,0.0003,0.0004,0.0005,0.0006,0.0007,0.0008,0.0009,0.001]#np.linspace(0.0001,0.001,10)\n",
    "num_factor_arr=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]\n",
    "nr_epochs=100\n",
    "min_epoch=30\n",
    "es_buffer=10 #Stop when the validation error is larger than the value 10 before it\n",
    "error_dict={}\n",
    "\n",
    "for learning_rate in learning_rate_arr:\n",
    "    for num_factor in num_factor_arr:\n",
    "        fm = pylibfm.FM(num_cols=np.max(X_train.indices) + 1, num_factor=num_factor, task=\"regression\", learning_rate=learning_rate)\n",
    "        train_error=[]\n",
    "        val_error=[]\n",
    "        early_stopping = False\n",
    "        nr_epochs=0\n",
    "        while early_stopping==False:\n",
    "        #for i in range(nr_epochs):\n",
    "            fm.learn(X_train, y_train)\n",
    "            preds_train = fm.predict(X_train)\n",
    "            preds_val = fm.predict(X_val)\n",
    "            train_error_tmp = mean_squared_error(y_train, preds_train,squared=False)    # RMSE\n",
    "            val_error_tmp = mean_squared_error(y_val, preds_val,squared=False)        # RMSE\n",
    "            train_error.append(train_error_tmp)\n",
    "            val_error.append(val_error_tmp)\n",
    "            if nr_epochs>min_epoch:\n",
    "                if val_error_tmp>val_error[-es_buffer]:\n",
    "                    early_stopping=True\n",
    "            print(\"Train FM RMSE: %.4f Validation FM RMSE: %.4f Epoch: %d\" % (train_error_tmp, val_error_tmp, nr_epochs))\n",
    "            nr_epochs+=1\n",
    "        error_dict[str(learning_rate)+\"_\"+str(num_factor)+\"_train\"]=train_error\n",
    "        error_dict[str(learning_rate)+\"_\"+str(num_factor)+\"_val\"]=val_error\n",
    "        save_error(train_error,val_error,X_filename,num_factor,learning_rate,nr_epochs)\n",
    "        save_fig(train_error,val_error,X_filename,num_factor,learning_rate,nr_epochs)\n",
    "        print(\"Best val RMSE is:\",np.min(val_error),\"at epoch\",np.argmin(val_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path=\"error/\"\n",
    "low_val_arr=np.zeros((len(learning_rate_arr)*len(num_factor_arr),4))\n",
    "i=0\n",
    "for learning_rate in learning_rate_arr:\n",
    "    for num_factor in num_factor_arr:\n",
    "        txt_filename=\"train_val_error_NF\"+str(num_factor)+\"_LR\"+str(learning_rate)\n",
    "        for filename in glob.glob(path+txt_filename+\"*.txt\"):\n",
    "            #print(filename)\n",
    "            epoch,low_val = lowest_val_error_from_txt(filename)\n",
    "            low_val_arr[i,:] = [low_val,learning_rate,num_factor,epoch]\n",
    "            i+=1\n",
    "print(low_val_arr[np.argmax(low_val_arr[:,-1])])\n",
    "print(min(low_val_arr[:,-1]))\n",
    "#plot_gridsearch(low_val_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now use the test set on optimal parameters\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from pyfm import pylibfm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_filename = 'user_encoded-movies_encoded-genre_matrix.npz' # User|movies|Genres\n",
    "\n",
    "\n",
    "X = sparse.load_npz(X_filename)\n",
    "filename=\"ml-latest-small/ratings.csv\"\n",
    "data=np.genfromtxt(filename,skip_header=1,delimiter=\",\")\n",
    "y = data[:,-2]\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "print(\"Training size: %d, Test size: %d\" % (X_train.shape[0],X_test.shape[0]))\n",
    "\n",
    "\n",
    "\n",
    "num_factor=13\n",
    "learning_rate=0.0005\n",
    "#learning_rate_arr=[0.0005]#np.linspace(0.0001,0.001,10)\n",
    "#num_factor_arr=[13]\n",
    "nr_epochs=62 # Derived from testing\n",
    "#min_epoch=30\n",
    "#es_buffer=10 #Stop when the validation error is larger than the value 10 before it\n",
    "error_dict={}\n",
    "\n",
    "\n",
    "fm = pylibfm.FM(num_cols=np.max(X_train.indices) + 1, num_factor=num_factor, task=\"regression\", learning_rate=learning_rate)\n",
    "train_error=[]\n",
    "test_error=[]\n",
    "early_stopping = False\n",
    "\n",
    "for i in range(nr_epochs):\n",
    "    fm.learn(X_train, y_train)\n",
    "    preds_train = fm.predict(X_train)\n",
    "    preds_test = fm.predict(X_test)\n",
    "    train_error_tmp = mean_squared_error(y_train, preds_train,squared=False)    # RMSE\n",
    "    test_error_tmp = mean_squared_error(y_test, preds_test,squared=False)        # RMSE\n",
    "    train_error.append(train_error_tmp)\n",
    "    test_error.append(test_error_tmp)\n",
    "    print(\"Train FM RMSE: %.4f Test FM RMSE: %.4f Epoch: %d\" % (train_error_tmp, test_error_tmp, i))\n",
    "    #nr_epochs+=1\n",
    "error_dict[str(learning_rate)+\"_\"+str(num_factor)+\"_train\"]=train_error\n",
    "error_dict[str(learning_rate)+\"_\"+str(num_factor)+\"_test\"]=test_error\n",
    "#save_error(train_error,test_error,X_filename,num_factor,learning_rate,nr_epochs)\n",
    "#save_fig_final(train_error,test_error,X_filename,num_factor,learning_rate,nr_epochs)\n",
    "print(\"Best test RMSE is:\",np.min(test_error),\"at epoch\",np.argmin(test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "def round_off_rating(preds):\n",
    "    r2=[]\n",
    "    for r in preds:\n",
    "        r2.append(round(r * 2) / 2)\n",
    "    return r2\n",
    "def bins_labels(bins, **kwargs):\n",
    "    bin_w = (max(bins) - min(bins)) / (len(bins) - 1)\n",
    "    plt.xticks(np.arange(min(bins)+bin_w/2, max(bins), bin_w), bins, **kwargs)\n",
    "    plt.xlim(bins[0], bins[-1])\n",
    "def overlay_distribution(r1,r2):\n",
    "    \n",
    "    u1, inv1 = np.unique(r1, return_inverse=True)\n",
    "    u2, inv2 = np.unique(r2, return_inverse=True)\n",
    "\n",
    "    counts1 = np.bincount(inv1)\n",
    "    counts2 = np.bincount(inv2)\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.grid(axis=\"y\",zorder=0)\n",
    "\n",
    "    plt.bar(u1, counts1/len(r1), width=0.5,edgecolor='black',alpha=0.5,label=\"Train-set Ratings\",zorder=1)#,bins_labels='r1')\n",
    "    plt.bar(u2, counts2/len(r2), width=0.5,edgecolor='black',alpha=0.5,label=\"Test-set Ratings\",color=\"r\",zorder=2)#,bins_labels='r2')\n",
    "\n",
    "\n",
    "    #n, bins, patches=plt.hist(ratings,weights=np.ones(len(ratings)) / len(ratings),edgecolor='black', linewidth=1.2,align=\"left\")\n",
    "    #plt.bar(ratings)\n",
    "    #ticklabels=(\"0.5\",\"1\",\"1.5\",\"2\",\"2.5\",\"3\",\"3.5\",\"4\",\"4.5\",\"5\")\n",
    "    #tick_pos=[0.5,1,1.5,2,2.5,3,3.5,4,4.5,5]\n",
    "    #ax.set_xticks(tick_pos)\n",
    "    #ax.set_xticklabels( ticklabels )\n",
    "    plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "    plt.xlabel(\"Rating\")\n",
    "    plt.ylabel(\"Percentage of Occurrences\")\n",
    "    plt.legend()\n",
    "    #plt.savefig(\"ratings_overlay_distribution_train_test.pdf\")\n",
    "    plt.show()\n",
    "def confusion_matrix_plot(target,pred):\n",
    "    cm = confusion_matrix(target, pred)\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, fmt='g', ax=ax,cmap='seismic');  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
    "\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted ratings(rounded to half-integers)');ax.set_ylabel('True ratings'); \n",
    "    #ax.set_title('Confusion Matrix'); \n",
    "    ax.xaxis.set_ticklabels([\"0\",\"0.5\",\"1\",\"1.5\",\"2\",\"2.5\",\"3\",\"3.5\",\"4\",\"4.5\",\"5\"])\n",
    "    ax.yaxis.set_ticklabels([\"0\",\"0.5\",\"1\",\"1.5\",\"2\",\"2.5\",\"3\",\"3.5\",\"4\",\"4.5\",\"5\"])\n",
    "    plt.savefig(\"confusion_matrix_best_model.pdf\")\n",
    "\"\"\"\n",
    "def get_confusion_statistics(target,pred):\n",
    "    cms = multilabel_confusion_matrix(target,pred)\n",
    "    cms=np.array(cms)\n",
    "    tn=cms[1:,0,0]\n",
    "    fn=cms[1:,1,0]\n",
    "    tp=cms[1:,1,1]\n",
    "    fp=cms[1:,0,1]\n",
    "    recall=tp/(tp+fn)\n",
    "    precision=tp/(tp+fp)\n",
    "    accuracy=(tn+tp)/(tn+fp+tp+fn)\n",
    "    print(cms.shape)\n",
    "    return recall, precision, accuracy\n",
    "\n",
    "def plot_confusion_statistics(recall,precision,accuracy):\n",
    "    r=[0.5,1,1.5,2,2.5,3,3.5,4,4.5,5]\n",
    "    plt.plot(r,recall,label=\"Recall Rate\")\n",
    "    plt.plot(r,precision,label=\"Precision\")\n",
    "    plt.plot(r,accuracy,label=\"Accuracy\")\n",
    "    plt.xticks(r,labels=[\"0.5\",\"1\",\"1.5\",\"2\",\"2.5\",\"3\",\"3.5\",\"4\",\"4.5\",\"5\"])\n",
    "    plt.xlabel(\"Ratings\")\n",
    "    plt.legend()\n",
    "    plt.ylabel(\"\")\n",
    "\"\"\"\n",
    "def recall_rate_plot(target,pred):\n",
    "    cm = confusion_matrix(target, pred)\n",
    "    cm=cm[1:,1:]\n",
    "    print(cm.shape)\n",
    "    recalls=[]\n",
    "    for i,line in enumerate(cm[:,:]):\n",
    "        recalls.append(cm[i,i])\n",
    "        recalls[-1]=recalls[-1]/sum(cm[i,:])\n",
    "    ratings=[0.5,1,1.5,2,2.5,3,3.5,4,4.5,5]\n",
    "    print(ratings)\n",
    "    print(recalls)\n",
    "    plt.grid()\n",
    "    plt.scatter(ratings,recalls,label=\"Recall Rate\")\n",
    "    plt.xlabel(\"Ratings\")\n",
    "    plt.xticks(ratings)\n",
    "    plt.ylabel(\"Recall Rate\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"recall_rate.pdf\")\n",
    "    #return recalls\n",
    "\n",
    "\n",
    "#r1=y_train\n",
    "r1=y_test\n",
    "#r1=y_test\n",
    "preds=fm.predict(X_test)\n",
    "preds=preds*2\n",
    "r1=r1*2\n",
    "r1=round_off_rating(r1)\n",
    "r2=round_off_rating(preds)\n",
    "r1=np.array(r1)\n",
    "r2=np.array(r2)\n",
    "\n",
    "r1=r1.astype(int)\n",
    "\n",
    "r2=r2.astype(int)\n",
    "print(r1)\n",
    "print(r2)\n",
    "#print(r1)\n",
    "#print(preds)\n",
    "#print(r2)\n",
    "#overlay_distribution(r1,r2)\n",
    "#confusion_matrix_plot(r1,r2)\n",
    "recall_rate_plot(r1,r2)\n",
    "#plt.plot(recall)\n",
    "#recall,precision,accuracy = get_confusion_statistics(r1,r2)\n",
    "#plot_confusion_statistics(recall,precision,accuracy)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc9146e8c65fd4b2f8e23fb981b3300af42b20615e04bdabe45c307201239cc5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
